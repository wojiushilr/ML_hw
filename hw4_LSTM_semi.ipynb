{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4_LSTM_semi.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPyitcb5oZAmGqHJ+AbK2Tg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wojiushilr/ML_hw/blob/master/hw4_LSTM_semi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MsmH-mUqWy8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ca0f7f1a-36cc-4926-982b-5b4227684efe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hm873wa-sml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "a6018e9f-192d-4780-c930-f93d2958da82"
      },
      "source": [
        "! pip install torch==1.4.0 torchvision==0.5.0"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 22kB/s \n",
            "\u001b[?25hCollecting torchvision==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 27.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.18.5)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.5.0\n",
            "    Uninstalling torch-1.5.0:\n",
            "      Successfully uninstalled torch-1.5.0\n",
            "  Found existing installation: torchvision 0.6.0\n",
            "    Uninstalling torchvision-0.6.0:\n",
            "      Successfully uninstalled torchvision-0.6.0\n",
            "Successfully installed torch-1.4.0 torchvision-0.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-AQwSkOEsIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgpXqJuZQKzD",
        "colab_type": "text"
      },
      "source": [
        "### load the pre-train w2v model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkCUYSjJeOkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/drive/My Drive/kaggle/hw4/ml2020spring-hw4.zip\" /content\n",
        "!cp \"/content/drive/My Drive/kaggle/hw4/w2v_all.model\" /content\n",
        "! cp \"/content/drive/My Drive/kaggle/hw4/w2v_all.model.wv.vectors.npy\" /content\n",
        "! cp \"/content/drive/My Drive/kaggle/hw4/w2v_all.model.trainables.syn1neg.npy\" /content"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ1uHvsaQSvO",
        "colab_type": "text"
      },
      "source": [
        "### load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBz1UUJFeZ5G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4ec0e752-37fb-493b-d384-4cf385699b2f"
      },
      "source": [
        "!ls\n",
        "!unzip ml2020spring-hw4.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\t      sample_data    w2v_all.model.trainables.syn1neg.npy\n",
            "ml2020spring-hw4.zip  w2v_all.model  w2v_all.model.wv.vectors.npy\n",
            "Archive:  ml2020spring-hw4.zip\n",
            "  inflating: testing_data.txt        \n",
            "  inflating: training_label.txt      \n",
            "  inflating: training_nolabel.txt    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6KktBM9e439",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cf6562ae-88a5-4f96-dec8-5565b7b216b5"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\t      training_nolabel.txt\n",
            "ml2020spring-hw4.zip  w2v_all.model\n",
            "sample_data\t      w2v_all.model.trainables.syn1neg.npy\n",
            "testing_data.txt      w2v_all.model.wv.vectors.npy\n",
            "training_label.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVhGIqt7Hik0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 设置后可以过滤一些无用的warning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AN7TciOe9mH",
        "colab_type": "text"
      },
      "source": [
        "### 這個 block 用來先定義一些等等常用到的函式\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt2gvgwXfEPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_training_data(path='training_label.txt'):\n",
        "    if \"training_label\" in path:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
        "            print(lines[0])\n",
        "        x = [line[2:] for line in lines]\n",
        "        y = [line[0] for line in lines]\n",
        "        # print(x[0])\n",
        "        # print(y[0])\n",
        "        return x, y\n",
        "    else:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            x = [line.strip('\\n').split(' ') for line in lines]\n",
        "        return x\n",
        "\n",
        "def load_testing_data(path='testing_data'):\n",
        "    # 把 testing 時需要的 data 讀進來\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n",
        "        X = [sen.split(' ') for sen in X]\n",
        "        print(X[0])\n",
        "    return X\n",
        "\n",
        "def evaluation(outputs, labels):\n",
        "    # outputs => probability (float)\n",
        "    # labels => labels\n",
        "    outputs[outputs>=0.5] = 1 # 大於等於 0.5 為正面\n",
        "    outputs[outputs<0.5] = 0 # 小於 0.5 為負面\n",
        "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
        "    return correct"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKqcep2qfNer",
        "colab_type": "text"
      },
      "source": [
        "### 這個 block 是用來訓練 word to vector 的 word embedding\n",
        "### 预计embedding耗时10min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5rgQ10tfJ6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# '''\n",
        "# 读取数据\n",
        "# w2v.py\n",
        "# 這個 block 是用來訓練 word to vector 的 word embedding\n",
        "# '''\n",
        "# import os\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import argparse\n",
        "# from gensim.models import Word2Vec\n",
        "# def train_word2vec(x):\n",
        "#     # 訓練 word to vector 的 word embedding\n",
        "#     model = Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)\n",
        "#     return model\n",
        "# if __name__ == \"__main__\":\n",
        "#     print(\"loading training data ...\")\n",
        "#     train_x, y = load_training_data('training_label.txt')\n",
        "#     train_x_no_label = load_training_data('training_nolabel.txt')\n",
        "\n",
        "#     print(\"loading testing data ...\")\n",
        "#     test_x = load_testing_data('testing_data.txt')\n",
        "\n",
        "#     # model = train_word2vec(train_x + train_x_no_label + test_x)\n",
        "#     model = train_word2vec(train_x + test_x)\n",
        "\n",
        "#     print(\"saving model ...\")\n",
        "#     # model.save(os.path.join(path_prefix, 'model/w2v_all.model'))\n",
        "#     model.save(os.path.join('w2v_all.model'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVzFNMORfc1E",
        "colab_type": "text"
      },
      "source": [
        "### 数据处理\n",
        "### NLP类似问题可以重复使用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwKKBZg_fh5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocess():\n",
        "    def __init__(self, sen_len, w2v_path=\"./w2v.model\"):\n",
        "        self.w2v_path = w2v_path   # word2vec的存储路径\n",
        "        self.sen_len = sen_len    # 句子的固定长度\n",
        "        self.idx2word = []\n",
        "        self.word2idx = {}\n",
        "        self.embedding_matrix = []\n",
        "    def get_w2v_model(self):\n",
        "        # 把之前訓練好的 word to vec 模型讀進來\n",
        "        self.embedding = Word2Vec.load(self.w2v_path)\n",
        "        self.embedding_dim = self.embedding.vector_size\n",
        "    def add_embedding(self, word):\n",
        "        # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector\n",
        "        vector = torch.empty(1, self.embedding_dim)\n",
        "        torch.nn.init.uniform_(vector) #  均匀分布\n",
        "        self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word.append(word)\n",
        "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0) # 指定维度为0，所以按行拼接\n",
        "\n",
        "    def make_embedding(self, load=True):\n",
        "        print(\"Get embedding ...\")\n",
        "        # 取得訓練好的 Word2vec word embedding\n",
        "        if load:\n",
        "            print(\"loading word to vec model ...\")\n",
        "            self.get_w2v_model()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # 製作一個 word2idx 的 dictionary\n",
        "        # 製作一個 idx2word 的 list\n",
        "        # 製作一個 word2vector 的 list\n",
        "        for i, word in enumerate(self.embedding.wv.vocab):\n",
        "            print('get words #{}'.format(i + 1), end='\\r')\n",
        "            # e.g. self.word2index['he'] = 1\n",
        "            # e.g. self.index2word[1] = 'he'\n",
        "            # e.g. self.vectors[1] = 'he' vector\n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "            self.idx2word.append(word)\n",
        "            self.embedding_matrix.append(self.embedding[word])\n",
        "        print('')\n",
        "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
        "        # 將 \"<PAD>\" 跟 \"<UNK>\" 加進 embedding 裡面\n",
        "        self.add_embedding(\"<PAD>\")\n",
        "        self.add_embedding(\"<UNK>\")\n",
        "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
        "        return self.embedding_matrix\n",
        "\n",
        "    def pad_sequence(self, sentence):\n",
        "        # 将每个句子变成一样的长度，即 sen_len 的长度\n",
        "        if len(sentence) > self.sen_len:\n",
        "        # 如果句子长度大于 sen_len 的长度，就截断\n",
        "            sentence = sentence[:self.sen_len]\n",
        "        else:\n",
        "        # 如果句子长度小于 sen_len 的长度，就补上 <PAD> 符号，缺多少个单词就补多少个 <PAD> \n",
        "            pad_len = self.sen_len - len(sentence)\n",
        "            for _ in range(pad_len):\n",
        "                sentence.append(self.word2idx[\"<PAD>\"])\n",
        "        assert len(sentence) == self.sen_len\n",
        "        return sentence\n",
        "    def sentence_word2idx(self, sentences):\n",
        "        sentence_list = []\n",
        "        for i, sen in enumerate(sentences):\n",
        "            print('sentence count #{}'.format(i+1), end='\\r')\n",
        "            sentence_idx = []\n",
        "            for word in sen:\n",
        "                if (word in self.word2idx.keys()):\n",
        "                    sentence_idx.append(self.word2idx[word])\n",
        "                else:\n",
        "                # 没有出现过的单词就用 <UNK> 表示\n",
        "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
        "            # 将每个句子变成一样的长度\n",
        "            sentence_idx = self.pad_sequence(sentence_idx)\n",
        "            sentence_list.append(sentence_idx)\n",
        "        return torch.LongTensor(sentence_list)\n",
        "    def labels_to_tensor(self, y):\n",
        "        # 把 labels 轉成 tensor\n",
        "        y = [int(label) for label in y]\n",
        "        return torch.LongTensor(y)\n",
        "    def get_pad(self):\n",
        "      return self.word2idx[\"<PAD>\"]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwCxXkArflA-",
        "colab_type": "text"
      },
      "source": [
        "### 重写 dataset 所需要的 '__init__', '__getitem__', '__len__'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKnB2ahEfqgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "data.py\n",
        "dataset 所需要的 '__init__', '__getitem__', '__len__'\n",
        "之后配合 dataloader 使用\n",
        "'''\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "class myDataset(Dataset):\n",
        "    def __init__(self,X,y):\n",
        "        self.data = X\n",
        "        self.label = y\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label == None:\n",
        "            return  self.data[idx]\n",
        "        else:\n",
        "            return  self.data[idx], self.label[idx]\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeQjxwSSfzmU",
        "colab_type": "text"
      },
      "source": [
        "### 模型定义"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB-iFwglfv6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "model.py\n",
        "模型定义\n",
        "'''\n",
        "import torch\n",
        "from torch import nn\n",
        "class myLSTM(nn.Module):\n",
        "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
        "        super(myLSTM,self).__init__()\n",
        "        # 初始化 embedding lookup表，\n",
        "        self.embed = torch.nn.Embedding(embedding.size(0),embedding.size(1)) # 0 -> 词数，1 -> 向量的大小\n",
        "        # 给上纪的表赋予之前处理得到的embedding，不给weight赋值的话就是随即值\n",
        "        self.embed.weight = torch.nn.Parameter(embedding)\n",
        "        # 是否將 embedding fix 住，如果 fix_embedding 為 False，在訓練過程中，embedding 也會跟著被訓練\n",
        "        if fix_embedding:\n",
        "            self.embed.weight.requires_grad = False\n",
        "        else:\n",
        "            self.embed.weight.requires_grad = True\n",
        "        # self.embed_dim = embedding.size(1)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.classifier = nn.Sequential(nn.Dropout(dropout),\n",
        "                                        nn.Linear(hidden_dim, 1),\n",
        "                                        nn.Sigmoid())\n",
        "    # 定义forward流程\n",
        "    def forward(self, inputs):\n",
        "        inputs = self.embed(inputs)\n",
        "        ##  此处不在意cn，hn所以为None\n",
        "        x, _ = self.lstm(inputs, None)\n",
        "        # 因为batch_first=True，所以 x 的 dimension (batch, seq_len, hidden_size)\n",
        "        # 取用 LSTM 最後一層的 hidden state\n",
        "        x = x[:, -1, :]\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2HKfKj5f1t-",
        "colab_type": "text"
      },
      "source": [
        "### 训练\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjb14jcUf4Ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "train.py\n",
        "'''\n",
        "## self-learning 把大于阀值的pred赋予label\n",
        "def add_label(outputs, threshold=0.9):\n",
        "    id = (outputs>=threshold) | (outputs<1-threshold)\n",
        "    outputs[outputs>=threshold] = 1 # 大于等于 threshold 为正面\n",
        "    outputs[outputs<1-threshold] = 0 # 小于 threshold 为负面\n",
        "    return outputs.long(), id\n",
        "\n",
        "def training(batch_size, n_epoch, lr,  X_train, y_train, train_x_no_label, train, valid, model, device):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
        "    loss = nn.BCELoss() # 定義損失函數，這裡我們使用 binary cross entropy loss\n",
        "    t_batch = len(train)\n",
        "    v_batch = len(valid)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)  # 將模型的參數給 optimizer，並給予適當的 learning rate\n",
        "    total_loss, total_acc, best_acc = 0, 0, 0\n",
        "    for epoch in range(n_epoch):\n",
        "        print(X_train.shape)\n",
        "        model.train()\n",
        "        total_loss, total_acc = 0, 0\n",
        "        # 這段做 training\n",
        "        for i, (inputs, labels) in enumerate(train):\n",
        "            # print(\"inputs_train: \", inputs)\n",
        "            inputs = inputs.to(device, dtype=torch.long)  # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
        "            labels = labels.to(device, dtype=torch.float)  # 將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
        "            optimizer.zero_grad()  # 由於 loss.backward() 的 gradient 會累加，所以每次餵完一個 batch 後需要歸零\n",
        "            outputs = model(inputs)  # 將 input 餵給模型\n",
        "            outputs = outputs.squeeze()  # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion(), 比如10 classes，output为(10,1),要把它转换为(10,)\n",
        "            batch_loss = loss(outputs, labels)  # 計算此時模型的 training loss\n",
        "            batch_loss.backward()  # 算 loss 的 gradient\n",
        "            optimizer.step()  # 更新訓練模型的參數\n",
        "            accuracy = evaluation(outputs, labels)  # 計算此時模型的 training accuracy\n",
        "            total_acc += (accuracy / batch_size)\n",
        "            total_loss += batch_loss.item()\n",
        "            # print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
        "            #     epoch + 1, i + 1, t_batch, loss.item(), correct * 100 / batch_size), end='\\r')\n",
        "        print('Epoch | {}/{}'.format(epoch+1,n_epoch))\n",
        "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss / t_batch, total_acc / t_batch * 100))\n",
        "\n",
        "        # 這段做 validation\n",
        "        model.eval()  # 將 model 的模式設為 eval，這樣 model 的參數就會固定住\n",
        "        # self-training\n",
        "        if epoch >= 4 :\n",
        "          train_no_label_dataset = myDataset(X=train_x_no_label, y=None) \n",
        "          train_no_label_loader = torch.utils.data.DataLoader(\n",
        "              train_no_label_dataset, \n",
        "              batch_size = batch_size, \n",
        "              shuffle = False, \n",
        "              num_workers = 0)\n",
        "          with torch.no_grad():\n",
        "             for i, (inputs) in enumerate(train_no_label_loader):\n",
        "              #  print(\"inputs: \", inputs)\n",
        "               inputs = inputs.to(device, dtype=torch.long) # 因为 device 为 \"cuda\"，将 inputs 转成 torch.cuda.LongTensor\n",
        "               outputs = model(inputs) # 模型输入Input，输出output\n",
        "               outputs = outputs.squeeze() # 去掉最外面的 dimension，好让 outputs 可以丢进 loss()\n",
        "              #  print(\"outputs: \", outputs)\n",
        "               labels, id = add_label(outputs)\n",
        "               # 加入新标注的数据\n",
        "               X_train = torch.cat((X_train.to(device), inputs[id]), dim=0)\n",
        "               y_train = torch.cat((y_train.to(device), labels[id]), dim=0)\n",
        "               if i == 0: \n",
        "                 train_x_no_label = inputs[~id]\n",
        "               else: \n",
        "                 train_x_no_label = torch.cat((train_x_no_label.to(device), inputs[~id]), dim=0)\n",
        "\n",
        "        # validation\n",
        "        if valid is None:\n",
        "            torch.save(model, \"ckpt.model\")\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                total_loss, total_acc = 0, 0\n",
        "            \n",
        "                for i, (inputs, labels) in enumerate(valid):\n",
        "                    inputs = inputs.to(device, dtype=torch.long) # 因为 device 为 \"cuda\"，将 inputs 转成 torch.cuda.LongTensor\n",
        "                    labels = labels.to(device, dtype=torch.float) # 因为 device 为 \"cuda\"，将 labels 转成 torch.cuda.FloatTensor，loss()需要float\n",
        "            \n",
        "                    outputs = model(inputs) # 模型输入Input，输出output\n",
        "                    outputs = outputs.squeeze() # 去掉最外面的 dimension，好让 outputs 可以丢进 loss()\n",
        "                    batch_loss = loss(outputs, labels) # 计算模型此时的 training loss\n",
        "                    accuracy = evaluation(outputs, labels) # 计算模型此时的 training accuracy\n",
        "                    total_acc += (accuracy / batch_size)\n",
        "                    total_loss += batch_loss.item()\n",
        "\n",
        "                v_batch = len(valid)\n",
        "                print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
        "                if total_acc > best_acc:\n",
        "                    # 如果 validation 的结果优于之前所有的結果，就把当下的模型保存下来，用于之后的testing\n",
        "                    best_acc = total_acc\n",
        "                    torch.save(model, \"ckpt.model\")\n",
        "        print('-----------------------------------------------')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS3Hjt_Ef7bo",
        "colab_type": "text"
      },
      "source": [
        "### 测试"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa18l4VBf84z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "test.py\n",
        "'''\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def testing(batch_size, test_loader, model, device):\n",
        "    model.eval()\n",
        "    ret_output = []\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(test_loader):\n",
        "            inputs = inputs.to(device, dtype=torch.long)\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.squeeze()\n",
        "            outputs[outputs>=0.5] = 1 # 大於等於 0.5 為正面\n",
        "            outputs[outputs<0.5] = 0 # 小於 0.5 為負面\n",
        "            ret_output += outputs.int().tolist()\n",
        "\n",
        "    return ret_output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPhfU32ff_tA",
        "colab_type": "text"
      },
      "source": [
        "### 主程序\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIgPBxp127vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m torch.utils.collect_env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uChnvV4t3AS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "72274867-c009-4c36-fef3-59d173cdac1c"
      },
      "source": [
        "! nvcc -V"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwYocrExgBRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "297783c5-2f33-4db5-c568-42c43d78ea01"
      },
      "source": [
        "'''\n",
        "main.py\n",
        "主程序\n",
        "'''\n",
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 通過 torch.cuda.is_available() 的回傳值進行判斷是否有使用 GPU 的環境，如果有的話 device 就設為 \"cuda\"，沒有的話就設為 \"cpu\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device: \",device)\n",
        "path_prefix ='./'\n",
        "\n",
        "# 處理好各個 data 的路徑\n",
        "train_with_label = os.path.join(path_prefix, 'training_label.txt')\n",
        "train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n",
        "testing_data = os.path.join(path_prefix, 'testing_data.txt')\n",
        "\n",
        "w2v_path = os.path.join('w2v_all.model') # 處理 word to vec model 的路徑\n",
        "\n",
        "# 定义句子长度、是否固定 embedding、batch 大小、epoch、learning rate 的值、model 的資料夾路徑等参数\n",
        "sen_len = 20\n",
        "fix_embedding = True # fix embedding during training\n",
        "batch_size = 128\n",
        "epoch = 10\n",
        "lr = 0.001\n",
        "\n",
        "model_dir = path_prefix # model directory for checkpoint model\n",
        "\n",
        "print(\"loading training data ...\") # 把 'training_label.txt' 跟 'training_nolabel.txt' 讀進來\n",
        "train_x, y = load_training_data(train_with_label)\n",
        "train_x_no_label = load_training_data(train_no_label)\n",
        "\n",
        "# 对于input和label做预处理\n",
        "# preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
        "# embedding = preprocess.make_embedding(load=True)\n",
        "# print(\"train_x[0]: \",train_x[0] )\n",
        "# train_x = preprocess.sentence_word2idx()\n",
        "# print(\"train_x[0] after sentence_word2idx: \",train_x[0] )\n",
        "# y = preprocess.labels_to_tensor(y)\n",
        "\n",
        "# 对 input 跟 labels 做预处理\n",
        "preprocess = Preprocess(sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "train_x = preprocess.sentence_word2idx(train_x)\n",
        "y = preprocess.labels_to_tensor(y)\n",
        "train_x_no_label = preprocess.sentence_word2idx(train_x_no_label)\n",
        "\n",
        "# import sys\n",
        "# sys.exit()\n",
        "\n",
        "# 製作一個 model 的對象\n",
        "model = myLSTM(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
        "model = model.to(device) # device為 \"cuda\"，model 使用 GPU 來訓練（餵進去的 inputs 也需要是 cuda tensor）\n",
        "\n",
        "# 把 data 分為 training data 跟 validation data（將一部份 training data 拿去當作 validation data）\n",
        "X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]\n",
        "\n",
        "# 把 data 做成 dataset 供 dataloader 取用\n",
        "train_dataset = myDataset(X=X_train, y=y_train)\n",
        "val_dataset =myDataset(X=X_val, y=y_val)\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = True,\n",
        "                                            num_workers = 8)\n",
        "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False,\n",
        "                                            num_workers = 8)\n",
        "# 開始訓練\n",
        "# def training(batch_size, n_epoch, lr,  X_train, y_train, train_x_no_label, train, valid, model, device):\n",
        "training(batch_size, epoch, lr, X_train, y_train, train_x_no_label, train_loader, val_loader, model, device)\n",
        "\n",
        "\n",
        "\n",
        "# 开始测试\n",
        "print(\"loading testing data ...\")\n",
        "test_x = load_testing_data(testing_data)\n",
        "\n",
        "# 对于input和label做预处理\n",
        "preprocess = Preprocess(sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "test_x = preprocess.sentence_word2idx(train_x)\n",
        "test_dataset = myDataset(X=test_x, y=None)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = False,\n",
        "                                            num_workers = 8)\n",
        "print('\\nload model ...')\n",
        "model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
        "outputs = testing(batch_size, test_loader, model, device)\n",
        "\n",
        "# 寫到 csv 檔案供上傳 Kaggle\n",
        "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n",
        "print(\"save csv ...\")\n",
        "tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n",
        "print(\"Finish Predicting\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device:  cuda\n",
            "loading training data ...\n",
            "['1', '+++$+++', 'are', 'wtf', '...', 'awww', 'thanks', '!']\n",
            "Get embedding ...\n",
            "loading word to vec model ...\n",
            "get words #55777\n",
            "total words: 55779\n",
            "\n",
            "start training, parameter total:14186101, trainable:241351\n",
            "\n",
            "torch.Size([180000, 20])\n",
            "Epoch | 1/10\n",
            "\n",
            "Train | Loss:0.49403 Acc: 75.160\n",
            "Valid | Loss:0.45327 Acc: 78.463 \n",
            "-----------------------------------------------\n",
            "torch.Size([180000, 20])\n",
            "Epoch | 2/10\n",
            "\n",
            "Train | Loss:0.43496 Acc: 79.689\n",
            "Valid | Loss:0.42911 Acc: 80.071 \n",
            "-----------------------------------------------\n",
            "torch.Size([180000, 20])\n",
            "Epoch | 3/10\n",
            "\n",
            "Train | Loss:0.41870 Acc: 80.615\n",
            "Valid | Loss:0.42185 Acc: 80.349 \n",
            "-----------------------------------------------\n",
            "torch.Size([180000, 20])\n",
            "Epoch | 4/10\n",
            "\n",
            "Train | Loss:0.40580 Acc: 81.387\n",
            "Valid | Loss:0.41782 Acc: 80.623 \n",
            "-----------------------------------------------\n",
            "torch.Size([180000, 20])\n",
            "Epoch | 5/10\n",
            "\n",
            "Train | Loss:0.39324 Acc: 81.994\n",
            "Valid | Loss:0.42131 Acc: 80.319 \n",
            "-----------------------------------------------\n",
            "torch.Size([619936, 20])\n",
            "Epoch | 6/10\n",
            "\n",
            "Train | Loss:0.38048 Acc: 82.691\n",
            "Valid | Loss:0.42615 Acc: 79.807 \n",
            "-----------------------------------------------\n",
            "torch.Size([713452, 20])\n",
            "Epoch | 7/10\n",
            "\n",
            "Train | Loss:0.36646 Acc: 83.426\n",
            "Valid | Loss:0.42703 Acc: 80.240 \n",
            "-----------------------------------------------\n",
            "torch.Size([775330, 20])\n",
            "Epoch | 8/10\n",
            "\n",
            "Train | Loss:0.35078 Acc: 84.240\n",
            "Valid | Loss:0.42523 Acc: 80.399 \n",
            "-----------------------------------------------\n",
            "torch.Size([809258, 20])\n",
            "Epoch | 9/10\n",
            "\n",
            "Train | Loss:0.33266 Acc: 85.227\n",
            "Valid | Loss:0.43280 Acc: 80.105 \n",
            "-----------------------------------------------\n",
            "torch.Size([841402, 20])\n",
            "Epoch | 10/10\n",
            "\n",
            "Train | Loss:0.31451 Acc: 86.115\n",
            "Valid | Loss:0.45880 Acc: 79.712 \n",
            "-----------------------------------------------\n",
            "loading testing data ...\n",
            "['my', 'dog', 'ate', 'our', 'dinner', '.', 'no', '', 'seriously', '...', 'he', 'ate', 'it', '.']\n",
            "Get embedding ...\n",
            "loading word to vec model ...\n",
            "get words #55777\n",
            "total words: 55779\n",
            "sentence count #200000\n",
            "load model ...\n",
            "save csv ...\n",
            "Finish Predicting\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}